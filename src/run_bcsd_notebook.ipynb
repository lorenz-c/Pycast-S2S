{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f75fbf-f52e-4e95-9e09-b122e6ac3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1b6ac-a41b-4a68-916c-b3d52a89ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xarray as xr\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import argparse\n",
    "\n",
    "from bc_module_v2 import bc_module\n",
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfa249-c108-4b69-a819-3b37cc864905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('bcsd_parameter.json')\n",
    "#parameter = json.load(f)\n",
    "    \n",
    "# Set month and year of the current forecast\n",
    "#month = parameter[\"issue_date\"][\"month\"]\n",
    "#year = parameter[\"issue_date\"][\"year\"]\n",
    "    \n",
    "# Convert the domain names in the parameter JSON to an array:\n",
    "#domain_names = [domain_names['name'] for domain_names in parameter[\"domains\"]]\n",
    "\n",
    "year        = '2022'\n",
    "month       = '04'\n",
    "domain_name = 'Khuzestan'\n",
    "\n",
    "file_params = {\n",
    "    'version': '2.2',\n",
    "    'glbldir': '/pd/data/regclim_data/gridded_data/seasonal_predictions/seas5/',\n",
    "    'regroot': '/pd/data/regclim_data/gridded_data/processed/'\n",
    "}\n",
    "\n",
    "bc_params = {\n",
    "    'dry_thresh': 0.01,\n",
    "    'low_extrapol': \"delta_additive\",\n",
    "    'up_extrapol': \"delta_additive\",\n",
    "    'extremes': \"weibull\",\n",
    "    'intermittency': True,\n",
    "    'nquants': 2500,\n",
    "    'window': 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b82b06-ee12-4005-86e1-98c2b744cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all filenames for in- and output files\n",
    "obs_dict, mdl_dict, pred_dict, month, bc_out_lns = modules.set_filenames(month, year, domain_name, file_params[\"regroot\"], file_params[\"version\"])\n",
    "\n",
    "# Read the dimensions for the output file (current prediction)\n",
    "coords = modules.get_coords_from_files(list(pred_dict.values())[0])\n",
    "\n",
    "# Set all the metadata for the output file\n",
    "global_attributes, variable_attributes = modules.set_metadata(coords, bc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a09df7-d967-4e5a-951c-4192dba84d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty NetCDF in which we write the BCSD output\n",
    "ds = modules.create_4d_netcdf(bc_out_lns, global_attributes, variable_attributes, coords)\n",
    "    \n",
    "# Load the NetCDF to get a handle for the output\n",
    "ds_out = xr.open_dataset(bc_out_lns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c4969-6141-46ed-8aef-1f1adde96e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some ressourcers\n",
    "client, cluster = modules.getCluster('haswell', 2, 40)\n",
    " \n",
    "# Do the memory magic...\n",
    "client.amm.start() \n",
    "    \n",
    "# Write some info about the cluster\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a986c-7e13-43b3-abbd-c19bef4710a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b268c6-e255-45fb-9a76-cc73c0c457f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Loop over each variable\n",
    "    for variable in variable_attributes:\n",
    "     \n",
    "\n",
    "        ###### Old IO-Module #####\n",
    "        # load data as dask objects\n",
    "        # Obs (1981 - 2016 on daily basis)\n",
    "        ds_obs = xr.open_mfdataset(obs_dict[variable], chunks={'time': 13149, 'lat': 50, 'lon': 50}, parallel=True, engine='h5netcdf')\n",
    "        ds_obs = ds_obs[variable].persist()\n",
    "        \n",
    "        # Mdl (historical, 1981 - 2016 for one month and 215 days)  215, 36, 25, 1, 1 ;\n",
    "        # Preprocess historical mdl-data, create a new time coord, which contain year and day at once and not separate\n",
    "        ds_mdl = modules.preprocess_mdl_hist(mdl_dict[variable], month) # chunks={'time': 215, 'year': 36, 'ens': 25, 'lat': 1, 'lon': 1})\n",
    "        ds_mdl = ds_mdl[variable].persist()\n",
    "        \n",
    "        # Pred (current year for one month and 215 days)\n",
    "        ds_pred = xr.open_mfdataset(pred_dict[variable], chunks={'time': 215, 'ens': 51, 'lat': 50, 'lon': 50}, parallel=True, engine='h5netcdf')\n",
    "        ds_pred = ds_pred[variable].persist()\n",
    "        \n",
    "        # Change data type of latidude and longitude, otherwise apply_u_func does not work\n",
    "        ds_pred = ds_pred.assign_coords(lon=ds_pred.lon.values.astype(np.float32), lat=ds_pred.lat.values.astype(np.float32))\n",
    "\n",
    "        # Calculate day of the year from time variable\n",
    "        dayofyear_obs = ds_obs['time.dayofyear']\n",
    "        dayofyear_mdl = ds_mdl['time.dayofyear']\n",
    "        \n",
    "        da_temp = xr.DataArray(\n",
    "            None, \n",
    "            dims = ['time', 'lat', 'lon', 'ens'], \n",
    "            coords = {\n",
    "                'time': ('time', coords['time'], {'standard_name': 'time', 'long_name': 'time'}),\n",
    "                'ens': ('ens', coords['ens'], {'standard_name': 'realization', 'long_name': 'ensemble_member'}),\n",
    "                'lat': ('lat', coords['lat'], {'standard_name': 'latitude', 'long_name': 'latitude', 'units': 'degrees_east'}),\n",
    "                'lon': ('lon', coords['lon'], {'standard_name': 'longitude', 'long_name': 'longitude', 'units': 'degrees_north'})\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if variable == \"tp\":\n",
    "            precip = True\n",
    "        else:\n",
    "            precip = False\n",
    "            \n",
    "        for timestep in range(0, len(ds_pred.time)):\n",
    "            \n",
    "            print(f'Correcting timestep {timestep}...')\n",
    "    \n",
    "        \n",
    "            day = dayofyear_mdl[timestep]\n",
    "    \n",
    "            day_range = (np.arange(day - bc_params['window'], day + bc_params['window'] + 1) + 365) % 365 + 1\n",
    "    \n",
    "            intersection_day_obs = np.in1d(dayofyear_obs, day_range)\n",
    "            intersection_day_mdl = np.in1d(dayofyear_mdl, day_range)\n",
    "    \n",
    "            ds_obs_sub = ds_obs.loc[dict(time=intersection_day_obs)]\n",
    "    \n",
    "            ds_mdl_sub = ds_mdl.loc[dict(time=intersection_day_mdl)]\n",
    "        \n",
    "            ds_mdl_sub = ds_mdl_sub.stack(ens_time=(\"ens\", \"time\"), create_index=True)\n",
    "            ds_mdl_sub = ds_mdl_sub.drop('time')\n",
    "    \n",
    "    \n",
    "            ds_pred_sub = ds_pred.isel(time=timestep)\n",
    "    \n",
    "    \n",
    "            pred_corr_act = xr.apply_ufunc(\n",
    "                bc_module, \n",
    "                ds_pred_sub, \n",
    "                ds_obs_sub, \n",
    "                ds_mdl_sub, \n",
    "                kwargs={'bc_params': bc_params, 'precip': precip},\n",
    "                input_core_dims=[[\"ens\"], [\"time\"], ['ens_time']], \n",
    "                output_core_dims=[[\"ens\"]], \n",
    "                vectorize=True, \n",
    "                dask=\"parallelized\", \n",
    "                output_dtypes=[np.float64]) \n",
    "        \n",
    "            da_temp.loc[dict(time=ds_pred.time.values[timestep])] = pred_corr_act\n",
    "        \n",
    "    \n",
    "        ds_out[variable] = da_temp.transpose('time', 'ens', 'lat', 'lon')\n",
    "        \n",
    "    # We still need to write the NetCDF..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06848faa-4a9c-446f-b9dc-1202cd92432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66697f8-78ea-44d3-a063-dbdebce3b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
